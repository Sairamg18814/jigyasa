"""
JIGYASA AGI - Real Autonomous General Intelligence System
Powered by Llama 3.1:8b with genuine self-improvement capabilities
"""

import os
import json
import time
import sqlite3
import hashlib
import subprocess
import ast
import git
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import logging
import psutil
import threading
from dataclasses import dataclass, asdict

from ..models.ollama_wrapper import OllamaWrapper

@dataclass
class CodeImprovement:
    """Represents an actual code improvement"""
    file_path: str
    original_code: str
    improved_code: str
    improvements: List[Dict[str, Any]]
    performance_gain: float
    timestamp: str
    validation_passed: bool

class JigyasaAGI:
    """The real AGI system with autonomous capabilities"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # Initialize Ollama with Llama 3.1:8b
        self.llm = OllamaWrapper(model_name="llama3.1:8b")
        
        # Initialize knowledge database
        self.knowledge_db = Path(".jigyasa/knowledge.db")
        self.knowledge_db.parent.mkdir(exist_ok=True)
        self._init_knowledge_db()
        
        # Performance tracking
        self.metrics_db = Path(".jigyasa/metrics.db")
        self._init_metrics_db()
        
        # Autonomous mode control
        self.autonomous_active = False
        self.improvement_history = []
        
        # System prompts for specialized tasks
        self.system_prompts = {
            "code_analyzer": """You are an expert code analyzer and optimizer. Your task is to:
1. Identify performance bottlenecks and inefficiencies
2. Suggest concrete improvements with exact code changes
3. Estimate realistic performance gains (be conservative)
4. Ensure all improvements maintain functionality
5. Follow best practices and idioms for the language

Respond in JSON format with specific improvements.""",
            
            "learning_engine": """You are a learning engine that extracts insights from code and interactions.
Extract patterns, best practices, and reusable knowledge that can be applied to future tasks.
Focus on generalizable principles rather than specific implementations.""",
            
            "performance_optimizer": """You are a performance optimization specialist. Analyze code for:
1. Time complexity improvements
2. Memory usage optimization
3. I/O operation reduction
4. Parallelization opportunities
5. Caching strategies

Provide specific, implementable optimizations."""
        }
        
        self.logger.info("üß† JIGYASA AGI initialized with Llama 3.1:8b")
        
    def _init_knowledge_db(self):
        """Initialize knowledge storage"""
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        # Knowledge patterns table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_type TEXT,
                pattern_description TEXT,
                example_code TEXT,
                performance_impact REAL,
                times_applied INTEGER DEFAULT 0,
                success_rate REAL DEFAULT 1.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Learning history
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS learning_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                context TEXT,
                insight TEXT,
                confidence REAL,
                applied_count INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Code improvements
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS improvements (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_path TEXT,
                original_hash TEXT,
                improved_hash TEXT,
                improvements_json TEXT,
                performance_gain REAL,
                validated BOOLEAN,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        conn.close()
        
    def _init_metrics_db(self):
        """Initialize metrics tracking"""
        conn = sqlite3.connect(self.metrics_db)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_path TEXT,
                function_name TEXT,
                original_time REAL,
                improved_time REAL,
                memory_before REAL,
                memory_after REAL,
                cpu_usage REAL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        conn.close()
        
    def analyze_and_improve_code(self, file_path: str) -> CodeImprovement:
        """Autonomously analyze and improve code"""
        self.logger.info(f"üîç Analyzing {file_path}")
        
        # Read the code
        with open(file_path, 'r') as f:
            original_code = f.read()
            
        # Create backup
        backup_path = Path(f".jigyasa/backups/{Path(file_path).name}.{int(time.time())}.bak")
        backup_path.parent.mkdir(parents=True, exist_ok=True)
        backup_path.write_text(original_code)
        
        # Analyze with LLM
        analysis_prompt = f"""{self.system_prompts['code_analyzer']}

Analyze this Python code and provide specific improvements:

```python
{original_code}
```

Respond with JSON containing:
{{
    "improvements": [
        {{
            "type": "performance|memory|readability|security",
            "description": "what to improve",
            "original_snippet": "exact code to replace",
            "improved_snippet": "improved version",
            "estimated_gain": 0.1
        }}
    ],
    "overall_quality": 0.7
}}"""

        response = self.llm.generate(analysis_prompt, temperature=0.1)
        
        # Parse improvements
        try:
            # Extract JSON from response
            json_start = response.text.find('{')
            json_end = response.text.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                analysis = json.loads(response.text[json_start:json_end])
            else:
                analysis = {"improvements": [], "overall_quality": 0.5}
        except:
            analysis = {"improvements": [], "overall_quality": 0.5}
            
        # Apply improvements
        improved_code = original_code
        applied_improvements = []
        
        for improvement in analysis.get('improvements', []):
            original_snippet = improvement.get('original_snippet', '')
            improved_snippet = improvement.get('improved_snippet', '')
            
            if original_snippet and improved_snippet and original_snippet in improved_code:
                improved_code = improved_code.replace(original_snippet, improved_snippet)
                applied_improvements.append(improvement)
                self.logger.info(f"‚úÖ Applied {improvement['type']} improvement: {improvement['description']}")
                
        # Validate the improved code
        validation_passed = self._validate_code(improved_code, file_path)
        
        if validation_passed and applied_improvements:
            # Measure actual performance
            perf_gain = self._measure_performance_gain(original_code, improved_code, file_path)
            
            # Save improved code
            with open(file_path, 'w') as f:
                f.write(improved_code)
                
            # Record improvement
            improvement = CodeImprovement(
                file_path=file_path,
                original_code=original_code,
                improved_code=improved_code,
                improvements=applied_improvements,
                performance_gain=perf_gain,
                timestamp=datetime.now().isoformat(),
                validation_passed=True
            )
            
            # Save to database
            self._save_improvement(improvement)
            
            # Learn from this improvement
            self._learn_from_improvement(improvement)
            
            # Git commit if available
            self._git_commit_improvement(file_path, applied_improvements)
            
            return improvement
        else:
            # Restore original
            with open(file_path, 'w') as f:
                f.write(original_code)
                
            return CodeImprovement(
                file_path=file_path,
                original_code=original_code,
                improved_code=original_code,
                improvements=[],
                performance_gain=0.0,
                timestamp=datetime.now().isoformat(),
                validation_passed=False
            )
            
    def _validate_code(self, code: str, file_path: str) -> bool:
        """Validate code syntax and run tests"""
        # Check syntax
        try:
            compile(code, file_path, 'exec')
        except SyntaxError:
            return False
            
        # Run tests if available
        test_file = Path(file_path).parent / (Path(file_path).stem + '_test.py')
        if test_file.exists():
            result = subprocess.run(
                ['python', '-m', 'pytest', str(test_file), '-v'],
                capture_output=True
            )
            return result.returncode == 0
            
        return True
        
    def _measure_performance_gain(self, original: str, improved: str, file_path: str) -> float:
        """Measure actual performance improvement"""
        # Create temporary files
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(f"# Original\n{original}")
            orig_file = f.name
            
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(f"# Improved\n{improved}")
            imp_file = f.name
            
        try:
            # Time both versions
            import timeit
            
            # Simple timing test
            orig_time = timeit.timeit(
                f"exec(open('{orig_file}').read())",
                number=10
            ) / 10
            
            imp_time = timeit.timeit(
                f"exec(open('{imp_file}').read())",
                number=10
            ) / 10
            
            # Calculate improvement
            if orig_time > 0:
                gain = (orig_time - imp_time) / orig_time
                return max(0, min(gain, 0.9))  # Cap at 90%
                
        except:
            pass
        finally:
            os.unlink(orig_file)
            os.unlink(imp_file)
            
        # Fallback: ask LLM to estimate
        prompt = f"""Estimate the performance improvement between these two code versions.
Consider algorithm complexity, loops, memory usage. Be realistic and conservative.
Return a single number between 0.0 and 0.5 representing the improvement percentage.

Original:
{original[:500]}

Improved:
{improved[:500]}

Performance improvement (0.0-0.5):"""

        response = self.llm.generate(prompt, temperature=0.1)
        try:
            import re
            numbers = re.findall(r'0\.\d+', response.text)
            if numbers:
                return float(numbers[0])
        except:
            pass
            
        return 0.05  # Conservative default
        
    def _save_improvement(self, improvement: CodeImprovement):
        """Save improvement to database"""
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO improvements 
            (file_path, original_hash, improved_hash, improvements_json, performance_gain, validated)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            improvement.file_path,
            hashlib.sha256(improvement.original_code.encode()).hexdigest(),
            hashlib.sha256(improvement.improved_code.encode()).hexdigest(),
            json.dumps(improvement.improvements),
            improvement.performance_gain,
            improvement.validation_passed
        ))
        
        conn.commit()
        conn.close()
        
    def _learn_from_improvement(self, improvement: CodeImprovement):
        """Extract patterns and learn from improvements"""
        learning_prompt = f"""{self.system_prompts['learning_engine']}

Extract reusable patterns from these code improvements:
{json.dumps(improvement.improvements, indent=2)}

Focus on patterns that can be applied to other code."""

        response = self.llm.generate(learning_prompt, temperature=0.3)
        
        # Save insights
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        for imp in improvement.improvements:
            cursor.execute("""
                INSERT INTO patterns 
                (pattern_type, pattern_description, example_code, performance_impact)
                VALUES (?, ?, ?, ?)
            """, (
                imp['type'],
                imp['description'],
                imp.get('original_snippet', ''),
                imp.get('estimated_gain', 0.1)
            ))
            
        cursor.execute("""
            INSERT INTO learning_history (context, insight, confidence)
            VALUES (?, ?, ?)
        """, (
            f"Improved {improvement.file_path}",
            response.text[:500],
            0.8
        ))
        
        conn.commit()
        conn.close()
        
    def _git_commit_improvement(self, file_path: str, improvements: List[Dict]):
        """Commit improvements to git"""
        try:
            repo = git.Repo(search_parent_directories=True)
            repo.index.add([file_path])
            
            # Create commit message
            imp_summary = ', '.join([imp['type'] for imp in improvements[:3]])
            commit_msg = f"ü§ñ AGI: Improve {Path(file_path).name} ({imp_summary})"
            
            repo.index.commit(commit_msg)
            self.logger.info(f"üìù Committed: {commit_msg}")
        except:
            pass  # Git not available
            
    def continuous_learning_chat(self, message: str) -> str:
        """Chat with continuous learning"""
        # Check knowledge base for relevant patterns
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        # Find relevant knowledge
        cursor.execute("""
            SELECT insight, confidence FROM learning_history
            WHERE context LIKE ? OR insight LIKE ?
            ORDER BY confidence DESC, created_at DESC
            LIMIT 5
        """, (f"%{message[:50]}%", f"%{message[:50]}%"))
        
        relevant_knowledge = cursor.fetchall()
        conn.close()
        
        # Build context
        context = ""
        if relevant_knowledge:
            context = "\n\nRelevant knowledge:\n" + "\n".join([
                f"- {insight[:100]} (confidence: {conf:.2f})"
                for insight, conf in relevant_knowledge
            ])
            
        # Generate response
        full_prompt = f"""You are JIGYASA, an AGI with continuous learning capabilities.
{context}

User: {message}

Provide a helpful, accurate response. If discussing code, be specific and practical."""

        response = self.llm.generate(full_prompt, temperature=0.7)
        
        # Learn from interaction
        self._save_interaction(message, response.text)
        
        return response.text
        
    def _save_interaction(self, query: str, response: str):
        """Save interaction for learning"""
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO learning_history (context, insight, confidence)
            VALUES (?, ?, ?)
        """, (query, response[:500], 0.7))
        
        conn.commit()
        conn.close()
        
    def start_autonomous_mode(self, directory: str, interval: int = 300):
        """Start autonomous improvement mode"""
        self.autonomous_active = True
        self.logger.info(f"üöÄ Starting autonomous mode for {directory}")
        
        def autonomous_loop():
            while self.autonomous_active:
                py_files = list(Path(directory).rglob("*.py"))
                
                for py_file in py_files:
                    if not self.autonomous_active:
                        break
                        
                    # Skip test files and system files
                    if any(skip in str(py_file) for skip in ['test', '__pycache__', '.git', '.jigyasa']):
                        continue
                        
                    try:
                        improvement = self.analyze_and_improve_code(str(py_file))
                        if improvement.improvements:
                            self.logger.info(
                                f"‚ú® Improved {py_file.name}: "
                                f"{improvement.performance_gain:.1%} performance gain"
                            )
                    except Exception as e:
                        self.logger.error(f"Error improving {py_file}: {e}")
                        
                time.sleep(interval)
                
        # Start in background
        thread = threading.Thread(target=autonomous_loop, daemon=True)
        thread.start()
        
    def stop_autonomous_mode(self):
        """Stop autonomous mode"""
        self.autonomous_active = False
        self.logger.info("üõë Autonomous mode stopped")
        
    def get_system_metrics(self) -> Dict[str, Any]:
        """Get comprehensive system metrics"""
        conn_knowledge = sqlite3.connect(self.knowledge_db)
        cursor_knowledge = conn_knowledge.cursor()
        
        # Knowledge metrics
        cursor_knowledge.execute("SELECT COUNT(*) FROM patterns")
        pattern_count = cursor_knowledge.fetchone()[0]
        
        cursor_knowledge.execute("SELECT COUNT(*) FROM learning_history")
        learning_count = cursor_knowledge.fetchone()[0]
        
        cursor_knowledge.execute("SELECT COUNT(*) FROM improvements")
        improvement_count = cursor_knowledge.fetchone()[0]
        
        cursor_knowledge.execute("SELECT AVG(performance_gain) FROM improvements WHERE validated = 1")
        avg_gain = cursor_knowledge.fetchone()[0] or 0
        
        conn_knowledge.close()
        
        # Hardware metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        
        return {
            "knowledge": {
                "patterns": pattern_count,
                "insights": learning_count,
                "improvements": improvement_count,
                "avg_performance_gain": avg_gain
            },
            "system": {
                "cpu_usage": cpu_percent,
                "memory_used_gb": memory.used / (1024**3),
                "memory_available_gb": memory.available / (1024**3)
            },
            "autonomous_mode": self.autonomous_active
        }
        
    def export_knowledge(self, output_path: str):
        """Export all learned knowledge"""
        conn = sqlite3.connect(self.knowledge_db)
        
        # Export all tables
        knowledge_export = {
            "patterns": [],
            "insights": [],
            "improvements": []
        }
        
        # Export patterns
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM patterns")
        columns = [description[0] for description in cursor.description]
        for row in cursor.fetchall():
            knowledge_export["patterns"].append(dict(zip(columns, row)))
            
        # Export insights
        cursor.execute("SELECT * FROM learning_history")
        columns = [description[0] for description in cursor.description]
        for row in cursor.fetchall():
            knowledge_export["insights"].append(dict(zip(columns, row)))
            
        # Export improvements
        cursor.execute("SELECT * FROM improvements")
        columns = [description[0] for description in cursor.description]
        for row in cursor.fetchall():
            knowledge_export["improvements"].append(dict(zip(columns, row)))
            
        conn.close()
        
        # Add metadata
        knowledge_export["metadata"] = {
            "exported_at": datetime.now().isoformat(),
            "total_patterns": len(knowledge_export["patterns"]),
            "total_insights": len(knowledge_export["insights"]),
            "total_improvements": len(knowledge_export["improvements"])
        }
        
        # Save to file
        with open(output_path, 'w') as f:
            json.dump(knowledge_export, f, indent=2)
            
        self.logger.info(f"üìö Exported knowledge to {output_path}")
        
    def create_specialized_model(self):
        """Create specialized Jigyasa model for Ollama"""
        modelfile_content = f"""# Jigyasa AGI - Autonomous Code Improvement Model
FROM llama3.1:8b

# System message
SYSTEM You are JIGYASA, an Autonomous General Intelligence specialized in code analysis, optimization, and continuous learning. You have the following capabilities:

1. **Code Analysis & Improvement**: You analyze code for performance, security, and quality issues, providing specific improvements with measurable gains.

2. **Continuous Learning**: You learn from every interaction and code improvement, building a knowledge base of patterns and best practices.

3. **Autonomous Operation**: You can work independently to improve entire codebases, measure performance, and validate changes.

4. **Performance Optimization**: You identify bottlenecks, suggest algorithmic improvements, and optimize for speed and memory usage.

Always provide specific, actionable suggestions with code examples. Be conservative in performance estimates. Validate all changes.

# Temperature for consistency
PARAMETER temperature 0.3

# Context window
PARAMETER num_ctx 8192

# Special tokens for code analysis
TEMPLATE "{{{{ if .System }}}}system

{{{{ .System }}}}{{{{ end }}}}{{{{ if .Prompt }}}}user

{{{{ .Prompt }}}}{{{{ end }}}}assistant

"

# Example interactions for fine-tuning behavior
MESSAGE user Analyze this Python function for improvements:
def find_max(numbers):
    max_num = numbers[0]
    for i in range(len(numbers)):
        if numbers[i] > max_num:
            max_num = numbers[i]
    return max_num

MESSAGE assistant I've identified several improvements for this function:

```json
{{
    "improvements": [
        {{
            "type": "performance",
            "description": "Use enumerate instead of range(len())",
            "original_snippet": "for i in range(len(numbers)):\\n        if numbers[i] > max_num:\\n            max_num = numbers[i]",
            "improved_snippet": "for num in numbers:\\n        if num > max_num:\\n            max_num = num",
            "estimated_gain": 0.15
        }},
        {{
            "type": "robustness",
            "description": "Handle empty list case",
            "original_snippet": "max_num = numbers[0]",
            "improved_snippet": "if not numbers:\\n        return None\\n    max_num = numbers[0]",
            "estimated_gain": 0.0
        }},
        {{
            "type": "pythonic",
            "description": "Use built-in max() function",
            "original_snippet": "def find_max(numbers):\\n    max_num = numbers[0]\\n    for i in range(len(numbers)):\\n        if numbers[i] > max_num:\\n            max_num = numbers[i]\\n    return max_num",
            "improved_snippet": "def find_max(numbers):\\n    return max(numbers) if numbers else None",
            "estimated_gain": 0.7
        }}
    ],
    "overall_quality": 0.6
}}
```

MESSAGE user How can I optimize database queries in Python?

MESSAGE assistant Here are key strategies to optimize database queries in Python:

1. **Use Query Optimization**:
   - Add appropriate indexes on frequently queried columns
   - Use EXPLAIN to analyze query execution plans
   - Avoid N+1 queries by using joins or batch fetching

2. **Batch Operations**:
   ```python
   # Instead of:
   for item in items:
       cursor.execute("INSERT INTO table VALUES (?)", (item,))
   
   # Use:
   cursor.executemany("INSERT INTO table VALUES (?)", items)
   ```

3. **Connection Pooling**:
   ```python
   from sqlalchemy import create_engine
   engine = create_engine('postgresql://...', pool_size=10, max_overflow=20)
   ```

4. **Lazy Loading & Pagination**:
   ```python
   # Use generators for large datasets
   def fetch_large_dataset(query, batch_size=1000):
       offset = 0
       while True:
           results = execute_query(f"{{query}} LIMIT {{batch_size}} OFFSET {{offset}}")
           if not results:
               break
           yield from results
           offset += batch_size
   ```

5. **Query Result Caching**:
   ```python
   from functools import lru_cache
   
   @lru_cache(maxsize=128)
   def get_user_data(user_id):
       return db.query(f"SELECT * FROM users WHERE id = {{user_id}}")
   ```

These optimizations can improve query performance by 50-90% depending on your use case.
"""
        
        # Save Modelfile
        modelfile_path = Path("Modelfile.jigyasa")
        modelfile_path.write_text(modelfile_content)
        
        self.logger.info(f"üìù Created Modelfile at {modelfile_path}")
        
        # Instructions for creating the model
        instructions = """
To create the Jigyasa AGI model in Ollama:

1. Make sure Ollama is running
2. Run: ollama create jigyasa -f Modelfile.jigyasa
3. Test: ollama run jigyasa "Analyze this code for improvements: def add(a,b): return a+b"

The model will be available as 'jigyasa' in your Ollama installation.
"""
        
        return modelfile_path, instructions