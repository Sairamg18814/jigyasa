#!/usr/bin/env python3
"""
Package JIGYASA for Hugging Face deployment
"""

import os
import json
import shutil
import subprocess
from pathlib import Path
import tempfile
import tarfile
from datetime import datetime

class HuggingFacePackager:
    def __init__(self):
        self.model_name = "jigyasa-agi"
        self.hf_username = "Sairamg18814"
        self.repo_name = f"{self.hf_username}/{self.model_name}"
        self.package_dir = Path("huggingface_package")
        
    def prepare_directory(self):
        """Prepare the packaging directory"""
        if self.package_dir.exists():
            shutil.rmtree(self.package_dir)
        self.package_dir.mkdir(parents=True)
        print(f"‚úÖ Created package directory: {self.package_dir}")
        
    def export_ollama_weights(self):
        """Export weights from Ollama model"""
        print("\nüì¶ Exporting Ollama model weights...")
        
        # Create a temporary GGUF file from Ollama
        export_script = '''#!/bin/bash
# Export JIGYASA model from Ollama
OLLAMA_MODELS_PATH="${HOME}/.ollama/models"
MODEL_MANIFEST="${OLLAMA_MODELS_PATH}/manifests/registry.ollama.ai/library/jigyasa/latest"

if [ -f "$MODEL_MANIFEST" ]; then
    echo "Found JIGYASA model manifest"
    # Copy model files
    cp -r "${OLLAMA_MODELS_PATH}/blobs/" ./ollama_blobs/
    echo "‚úÖ Exported model blobs"
else
    echo "‚ùå JIGYASA model not found in Ollama"
    exit 1
fi
'''
        
        with open("export_ollama.sh", "w") as f:
            f.write(export_script)
        
        os.chmod("export_ollama.sh", 0o755)
        
        # Note: Actual weight conversion would require model conversion tools
        print("‚ö†Ô∏è  Note: Full weight conversion requires additional tools")
        print("   For production, use llama.cpp or similar for GGUF to HF conversion")
        
    def create_model_files(self):
        """Create all necessary model files"""
        print("\nüìù Creating model files...")
        
        # Copy existing files
        files_to_copy = [
            ("huggingface/README.md", "README.md"),
            ("huggingface/config.json", "config.json"),
            ("huggingface/tokenizer_config.json", "tokenizer_config.json")
        ]
        
        for src, dst in files_to_copy:
            src_path = Path(src)
            dst_path = self.package_dir / dst
            if src_path.exists():
                shutil.copy2(src_path, dst_path)
                print(f"‚úÖ Copied {src} -> {dst}")
        
        # Create generation config
        generation_config = {
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "max_length": 2048,
            "repetition_penalty": 1.1,
            "do_sample": True,
            "use_cache": True,
            "_from_model_config": True,
            "bos_token_id": 128256,
            "eos_token_id": 128257,
            "pad_token_id": 128255,
            "transformers_version": "4.36.0"
        }
        
        with open(self.package_dir / "generation_config.json", "w") as f:
            json.dump(generation_config, f, indent=2)
        
        # Create special tokens map
        special_tokens = {
            "bos_token": {"content": "<|begin_of_text|>", "single_word": False, "lstrip": False, "rstrip": False, "normalized": False},
            "eos_token": {"content": "<|end_of_text|>", "single_word": False, "lstrip": False, "rstrip": False, "normalized": False},
            "unk_token": {"content": "<|unknown|>", "single_word": False, "lstrip": False, "rstrip": False, "normalized": False},
            "pad_token": {"content": "<|pad|>", "single_word": False, "lstrip": False, "rstrip": False, "normalized": False}
        }
        
        with open(self.package_dir / "special_tokens_map.json", "w") as f:
            json.dump(special_tokens, f, indent=2)
        
        print("‚úÖ Created all model configuration files")
        
    def create_example_usage(self):
        """Create example usage scripts"""
        print("\nüí° Creating usage examples...")
        
        # Python usage example
        python_example = '''#!/usr/bin/env python3
"""
Example usage of JIGYASA AGI model from Hugging Face
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer
model_name = "Sairamg18814/jigyasa-agi"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

def optimize_code(code: str) -> str:
    """Use JIGYASA to optimize Python code"""
    prompt = f"""<|im_start|>system
You are JIGYASA, an AGI specialized in code optimization.
<|im_end|>
<|im_start|>user
Optimize this Python code:

{code}
<|im_end|>
<|im_start|>assistant
"""
    
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=1000,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("<|im_start|>assistant")[-1].strip()

# Example usage
if __name__ == "__main__":
    code = """
def find_duplicates(items):
    duplicates = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            if items[i] == items[j] and items[i] not in duplicates:
                duplicates.append(items[i])
    return duplicates
"""
    
    print("Original code:")
    print(code)
    print("\nJIGYASA optimization:")
    print(optimize_code(code))
'''
        
        with open(self.package_dir / "example_usage.py", "w") as f:
            f.write(python_example)
        
        # CLI usage example
        cli_example = '''#!/bin/bash
# JIGYASA CLI Usage Examples

echo "üß† JIGYASA AGI - Code Optimization Examples"
echo "=========================================="

# Example 1: Optimize a function
curl -X POST "https://api-inference.huggingface.co/models/Sairamg18814/jigyasa-agi" \
  -H "Authorization: Bearer YOUR_HF_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"inputs": "Optimize this: def sum_list(items): total = 0; for item in items: total += item; return total"}'

# Example 2: Analyze performance
curl -X POST "https://api-inference.huggingface.co/models/Sairamg18814/jigyasa-agi" \
  -H "Authorization: Bearer YOUR_HF_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"inputs": "Analyze performance bottlenecks in nested loops"}'

# Example 3: Learn patterns
curl -X POST "https://api-inference.huggingface.co/models/Sairamg18814/jigyasa-agi" \
  -H "Authorization: Bearer YOUR_HF_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"inputs": "What optimization patterns work best for data processing?"}'
'''
        
        with open(self.package_dir / "cli_examples.sh", "w") as f:
            f.write(cli_example)
        
        os.chmod(self.package_dir / "cli_examples.sh", 0o755)
        print("‚úÖ Created usage examples")
        
    def create_model_card_data(self):
        """Create model card metadata"""
        model_card_data = {
            "language": ["en", "code"],
            "license": "mit",
            "library_name": "transformers",
            "tags": [
                "code",
                "code-generation", 
                "code-optimization",
                "autonomous-ai",
                "agi",
                "llama",
                "performance-optimization",
                "continuous-learning"
            ],
            "datasets": ["custom"],
            "metrics": [
                "code_execution_speed",
                "memory_usage",
                "algorithm_complexity"
            ],
            "model-index": [{
                "name": "jigyasa-agi",
                "results": [{
                    "task": {
                        "type": "code-optimization",
                        "name": "Code Optimization"
                    },
                    "metrics": [{
                        "type": "performance_gain",
                        "value": 65,
                        "name": "Average Performance Improvement"
                    }]
                }]
            }]
        }
        
        with open(self.package_dir / "model_card_data.json", "w") as f:
            json.dump(model_card_data, f, indent=2)
        print("‚úÖ Created model card metadata")
        
    def create_requirements(self):
        """Create requirements file for the model"""
        requirements = """transformers>=4.36.0
torch>=2.0.0
accelerate>=0.24.0
sentencepiece>=0.1.99
protobuf>=3.20.0
"""
        
        with open(self.package_dir / "requirements.txt", "w") as f:
            f.write(requirements)
        print("‚úÖ Created requirements.txt")
        
    def create_upload_script(self):
        """Create script to upload to Hugging Face"""
        upload_script = f'''#!/usr/bin/env python3
"""
Upload JIGYASA to Hugging Face Hub
"""

from huggingface_hub import HfApi, create_repo, upload_folder
import os

def upload_to_huggingface():
    # Initialize API
    api = HfApi()
    
    # Get token from environment
    token = os.getenv("HUGGINGFACE_TOKEN")
    if not token:
        print("‚ùå Please set HUGGINGFACE_TOKEN environment variable")
        return False
    
    # Create repository
    repo_id = "{self.repo_name}"
    try:
        create_repo(repo_id, token=token, repo_type="model", exist_ok=True)
        print(f"‚úÖ Repository created/exists: {{repo_id}}")
    except Exception as e:
        print(f"‚ùå Error creating repository: {{e}}")
        return False
    
    # Upload files
    try:
        upload_folder(
            folder_path="{self.package_dir}",
            repo_id=repo_id,
            token=token,
            commit_message="Upload JIGYASA AGI model v1.0.0"
        )
        print(f"‚úÖ Successfully uploaded to: https://huggingface.co/{{repo_id}}")
        return True
    except Exception as e:
        print(f"‚ùå Error uploading: {{e}}")
        return False

if __name__ == "__main__":
    upload_to_huggingface()
'''
        
        with open("upload_to_huggingface.py", "w") as f:
            f.write(upload_script)
        
        os.chmod("upload_to_huggingface.py", 0o755)
        print("‚úÖ Created upload script")
        
    def create_package_archive(self):
        """Create a tar.gz archive of the package"""
        archive_name = f"jigyasa-agi-{datetime.now().strftime('%Y%m%d')}.tar.gz"
        
        with tarfile.open(archive_name, "w:gz") as tar:
            tar.add(self.package_dir, arcname=self.model_name)
        
        print(f"‚úÖ Created archive: {archive_name}")
        return archive_name
        
    def package(self):
        """Main packaging process"""
        print("üì¶ JIGYASA Hugging Face Packager")
        print("=" * 50)
        
        # Prepare package
        self.prepare_directory()
        self.create_model_files()
        self.create_example_usage()
        self.create_model_card_data()
        self.create_requirements()
        self.create_upload_script()
        
        # Export weights (placeholder for now)
        self.export_ollama_weights()
        
        # Create archive
        archive = self.create_package_archive()
        
        print("\n‚úÖ Packaging complete!")
        print(f"\nüìÅ Package directory: {self.package_dir}")
        print(f"üì¶ Archive: {archive}")
        print("\nüì§ To upload to Hugging Face:")
        print("   1. Set your HF token: export HUGGINGFACE_TOKEN=your_token")
        print("   2. Run: python upload_to_huggingface.py")
        print("\nüéØ Model will be available at:")
        print(f"   https://huggingface.co/{self.repo_name}")
        
        return True

if __name__ == "__main__":
    packager = HuggingFacePackager()
    packager.package()